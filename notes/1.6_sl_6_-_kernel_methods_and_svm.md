# SL 6 - Kernel Methods and SVMs

## Overview

Support Vector Machines (SVMs) 

When considering a line to separate two groups of data in  a 2D plane (i.e., a group in the lower left and another in the upper right) we would want to consider the line the best performs a separation of the two groups, a *decision boundary*, all while committing as little as possible to either group.

![Decision boundary for SVM](https://upload.wikimedia.org/wikipedia/commons/1/1b/Kernel_Machine.png)

For the case with hyperplanes, our linear classifier can be defined as:

$$y=w^Tx+b$$

where:

- $y$ is the *classification label* and $y \in \{-1, +1\}$ with

$$ 
\begin{cases} 
\text{in class}  & \text{for  } y \gt 0  \\  
\text{out of class}  &\text{for  } y \lt 0 
\end{cases}
$$

- $w^T$ and $b$ are the parameters of the plane 

The case for the *decision boundary* (i.e., the red line in the figure above), the classifier is expressed as:

$$w^Tx+b=0$$

Similarly, the dashed lines / hyperplanes on both sides can be expressed as:

$$w^Tx+b=1$$

$$w^Tx+b=-1$$

In order to maximize the the distance between these two hyperplanes, we subtract the two equations above, as:

$$
\begin{equation}
\frac{w^T(x_1 - x_2)}{\left\Vert w \right\Vert
} = \frac{ 2 }{\left\Vert w \right\Vert}
\end{equation}
$$

This is known as the *margin*, defined as 

$$m = \frac{ 2 }{\left\Vert w \right\Vert}$$

## Still Support 

Therefore, the goal with the support vector machine is to maximize the *margin*,  $m$, subject to the constraint that we classify everything correctly. Together, this can be defined mathematically as:

$$ \textrm{max}(m) : y_i (w^T X_i +b) \ge 1 \; \forall_i $$

Mathematically, this is equivalent to 

$$\textrm{min} \left ( \frac{1}{2} \left\Vert w \right\Vert ^2 \right )~$$

which can now be expressed in a form with a well know solutionâ€”a quadratic equation.


$$
W( \alpha ) = \sum_i \alpha_i 
-\frac{1}{2} \sum_{ij} \alpha_i \alpha_i y_i y_j x_i^\textrm{T}x_j \\
\textrm{s.t.}: \alpha_i >= 0, \sum_i \alpha_i y_i = 0
$$

which now is now in the form of a quadratic function. The functions represent a special class of optimization problems with well-known solutions known as [quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming).

Additionally, a few other definitions, 

- $ w = \sum_i \alpha_i y_i x_i $
- $\alpha_i $ are mostly 0 $\rightarrow$ only a few of $x_i$ matter.
- $x_i^\textrm{T}x_j$ is a dot product, producing the projecting of $x_i$ onto $x_j$, and representing a [*similarity function*](https://en.wikipedia.org/wiki/Similarity_measure) (i.e., the inverse of a distance metric).

## SVMs: Linearly Married

### Transformations

Consider some plane of points, of two types, $x$ and $y$, where the $x$'s surround the $y$'s, with coordinates $(x_1, x_2)$ and $(y_1, y_2)$, respectively. Consider a general transformation of a point, $q$, with coordinates, $(q_1, q_2)$.

$$ \Phi (q) = < q_1^2, q_2^2,  \sqrt{2} q_1 q_2 > $$

This representation of $q$ contains no additional information, per se, however, now allows possibilities for separating the data linearly along the additional dimension represented in coordinate $ \sqrt{2} q_1 q_2 $. 

We consider the above in the following: given the above, find the solution to:

$$
\begin{equation}
\begin{split}
 \Phi (x)^\textrm{T}  \Phi (y) & = \\
 & = \left [x_1^2, x_2^2,  \sqrt{2} x_1 x_2 \right ]^\textrm{T} \left [ y _1^2, y_2^2,  \sqrt{2} y_1 y_2 \right ] \\
 & = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 \\
 & = ( x_1 y_1 + x_2 y_2 ) ^ 2 \\
 & = ( x^ \textrm{T} y ) ^ 2
 \end{split}
 \end{equation}
$$

What we observe is a *kernel trick*, demonstrating that the transformation of the original 2D data to the higher dimension can be computed in the *implicit feature space*, never requiring  computations on the coordinates to the transformed space. This is typically much more computationally efficient, and therefore desirable for learning endeavors.


### On the Value of Kernels 

This is an example of *similarity*. 

Ultimately, the above similarity transformation $ \Phi (x)^\textrm{T}  \Phi (y)$ allows for a representation of the data that *is* linearly separable by the SVM, in this instance, allowing for separating the data that was originally distributed radially, to be separated along another dimension.

When we reconsider how the *kernel trick* might be represented
$$
W( \alpha ) = \sum_i \alpha_i 
-\frac{1}{2} \sum_{ij} \alpha_i \alpha_i y_i y_j \textrm{K}(x_i, y_j )
$$

where $\textrm{K}$ represents the kernel function, which computes the measure of *similarity*. It is in this component that we are able to inject domain knowledge into the SVM.

### Other forms of Kernels

Kernels can be anything, but might often include:

- $K =  (x^T y)^2$
- $K =  x^T y$
- $K =  (x^T y + c)^P$, a polynomial.
- $K = e^{- \frac{1}{2 \sigma^2} || x-y ||^2}$, a radial basis function (guassian).
- $K = \tanh( \alpha x^T y + \Theta)$

They can also be discrete functions, categorical, as long as these represent some notion of similarity. For instance, it it could be strings, and some parsing. 

Kernels must to hold to the *[Mercer's condition](https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer.27s_condition)*. 


## Summary

Eager lazy learners

- **margins**: generalization & overfitting
- **maximizing margins**: bigger is better
- **optimization** problem for finding max margins: quadratic programming!
- **support vectors**: the points from the input data that are most important for defining the maximum margin separator. 
- **kernel trick**, $x^Ty$ can be represented by $ K(x, y) $, $ \rightarrow $ domain knowledge

## Back to Boosting. 

On the discussion of boostings impact on *overfitting*, we've been ignoring some information. 

What we normally keep track of is:

- *error* (i.e., the probability that you will come up with an answer that disagrees with your training set). 

however, what we are also keeping track of is *confidence*. 

$$ 
H(x)_{\text{final}} = 
\text{sgn} 
\left( 
\sum_t \alpha_t h_t(x) 
\right) 
$$

which simply outputs the sign of the sum over the weak hypothesis +1, - 1, or 0. Take the above and divide by the weights we used, normalizing the output:

$$ 
H(x)_{\text{final}} = 
\frac{ 
\left (
\text{sgn} \sum_t\alpha_t h_t(x) 
\right )}
{ \sum\alpha_t}
$$

This normalization reduces the output between -1 and +1, and in the process of adding more weak learners, the margin increases (e.g., the distance between boundaries). This increases the confidence while the actual error remains the same.

Boosting tends to overfit also if the underlying weak learner tends to overfit, as in the case of an artificial neural network.

 





