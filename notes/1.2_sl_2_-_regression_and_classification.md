# SL 2 - Regression & Classification


Supervised learning takes examples of input and output. SL builds a model that allows for passing *new* input and predicting its output.

## Regression 


Regressions maps *continuous* inputs to outputs, in comparison to *discrete* inputs and outputs.

Regression, as a term, stems from the observation that individual instances of any observed attribute tend to *regress* towards the mean.

Regression now refers mostly to *Function Approximation*.



## Errors

Training data has error, we are not modeling $f$ but rather $f + \epsilon$. 

Where do errors come from? Errors arise in any data set due to multiple reasons:

1. *Sensor Error* (machines making mistakes interpreting physical world)
2. *Unmodeled Influences* (unaccounted for) (e.g. other processes that have an impact on the data that is not represented in the data)
3. *Transcription Error* (Humans making mistakes in some part of data wrangling / transformation)
4. *Malicious Intent* (humans making intentional mistakes)


## Cross validation

Cross validation (CV) is an operation to quantify a model's generality / ability to predict future data. We assume, for cross validation purposes, that the training and testing data is representative of the world and IID (independent and identically distributed).

### Training 

Refers to the model fitting operation (e.g., regression) whereby model type, parameters, degrees of freedom are adjusted to "best fit" the data.  

Use a model that is complex enough to fit the data without causing problems on the test set. This is done by minimizing error between model and data by some error (e.g., loss) function.

### Spitting Data

Break apart the sample data into two ore more sets, training and testing. The training is used to describe the test set (e.g. the future or other real-life scenarios).

#### Folds
1. To pick a model...

2. Break the training data apart into four equally size folds.

For each cross validation exercise, we choose three of the four sets to train on, and compare the remaining fold with a testing exercise.

### Errors 

Considering $n$ degree of polynomial for model.

- training error always decreases as $n$ increase in the training
- cross validation error increases and then improves in the testing 

as $n$ increases: error first underfits $\rightarrow$ fits $\rightarrow$ overfits


## Other Input Spaces

### Scalar input, continuous: $x$

### Vector input, continuous: $\bar{x}$
 
Include more input features. In the case of the home data, this could be size and distance from zoo. Encoding discrete parameters (e.g., hair color) is an important consideration.



