# SL 3 - Neural Networks

[TOC]

## Neurons
Consist of:

- Cell Body
- Axon
- Synapses

Cell body receives signals via synapses from other neurons, and is able upon crossing some threshold of the combination of signals, activates and fires its own signal along the axon an across the synapses.
	
![Neuron](http://webspace.ship.edu/cgboer/neuron.gif)

## Artificial Neural Network

We consider a cartoonish version of the network of neurons, allowing for combinations to fire under different stimuli, allowing for learning.

![Perceptron Model](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)

### Perceptron Unit

To model a neuron, we consider an input $X_i$ with corresponding weights $w_i$ (or gains), that is a component of a set of multiple inputs $i \in 1, 2, ... k$, all feeding a *unit*. We consider the collective *strength* of the inputs by summing all of the products of the inputs and their *weights*, also known as the *activation*, defined as:

 $$a = \sum_{i = 1}^k  x_i  w_i$$. 
 The neuron fires if the activation exceeds a *firing threshold*, defined as $\Theta$. 

$$ 
y =  \begin{cases} 1 & \text{for  } a \gt \Theta  \\  0 &\text{for  } a \le \Theta 
\end{cases}
$$

This model is known as a *Perceptron*.


### Logical Operators

- *AND* as a perceptron unit
- *OR* as a perceptron unit
- *XOR* as a perceptron unit

## Perceptron Training 

Instead of setting weights by hand, we can train perceptrons.

Goal: Given examples, find weights that map inputs to outputs. 

Two method for training will be considered:

- perceptron rule (threshold)
- gradient decent / delta rule (unthreshold)

### Perceptron Rule $\rightarrow$ Single Unit


$$ w_{i+1} = w_i + \Delta w_i $$

$$ \Delta w_i = \eta ( y - \hat{y})X_i $$

$$ \hat{y} = \left( \sum_i w_i x_i \ge 0 \right) $$

where,

- $y$ : target
- $\hat{y}$ : output
- $\eta$ : learning rate
- $x$ : input

Through each example $i$, 

| $y$ | $\hat{y}$ | $y-\hat{y}$ |
| :------- | :---- | :---- |
| 0 | 0 |  0  |
| 0 | 1 |  -1 |
| 1 | 0 |  1  |
| 1 | 1 |  0  |


If the data are linearly separable, the perceptron rule will find it.


### Gradient Descent

We need a learning algorithm that is more robust to non-(====linear separability). 

For some *activation* energy:

$$
a = \sum_{i = 1}^k  x_i  w_i 
$$

we define an error metric as:

$$
E(w) = \dfrac{1}{2} \sum_{(x, y) \in D} \left (y-a \right )^2 
$$

to minimize, we take derivative of above like:

$$
\begin{align} 
\frac{\partial E}{\partial w_i} & = \frac{\partial}{\partial w_i} \left ( \dfrac{1}{2} \sum_{(x, y) \in D} \left (y-a \right )^2 \right ) \\
& = \sum_{(x, y) \in D} (y-a) \frac{\partial}{\partial w_i}  \left ( - \sum_{i'} x_i w_{i'} \right ) \\
& = \sum_{(x, y) \in D} (y-a)  \left ( - x_i  \right )
\end{align} 
$$

### Comparison of learning Rules

#### Perceptron 

- Guarantee finite convergence, however, only if linearly separable.

$\Delta w_i = \eta \left ( y - \hat{y} \right ) x_i$

#### Gradient Descent 
- Calculus-based
- More robust to data sets that are not linearly separable, however, converges to local minima / optima. 

$\Delta w_i = \eta \left ( y - a \right ) x_i$


## Sigmoid Function - Differentiable Threshold

$$ \sigma (a) = \frac{1}{1 + e^{-a}}$$ 

- as $ a \rightarrow -\infty$,   $ \sigma(a) \rightarrow 0$
- as $ a \rightarrow +\infty$,  $ \sigma(a) \rightarrow 1$

functionally, it works well due to being differentiable:

$ D( \sigma (a)) = \sigma (a) \left ( 1 - \sigma (a) \right ) $

## Back Propagation




### Optimizing Weights

Many consider *learning* to be a more a difficult higher-level for of optimization.

- momentums 
- higher order derivatives (Newtonians)
- randomized optimization
- penalty for complexity
	- number of nodes
	- more layers
	- weight sizes (large values induce more complexity)

## Restriction Bias

What are neural networks appropriate for?

The restriction bias tells us something about the representational power of the model by considering the set of hypothesis that we will consider.

Neural Networks consist of:

- Perceptron: half-spaces
- Sigmoids: (instead of step functions): much more complex
- Hidden Layers: (groups of sigmoid functions)

All of these components within a neural network allow for modeling many types of functions / behaviors, such as:

- Boolean: network of threshold-like units
- Continuous: through hidden layers (e.g. use of sigmoids instead of step)
- Arbitrary (non-continuous): multiple hidden layers 

**Conclusion**:  Neural networks have low restriction bias, because they can model *many different functions*. Therefore they have the danger of *overfitting*.

**Interestingly** in cross-validation efforts, one can observe the same characteristic plot of testing vs. training on each *iteration* of neural network, however, *this isn't changing the network structure (e.g. model / hypothesis as referred to in other ML algorithms)*. Instead it's simply a convergence on the weights of the input features.

**Conclusion**:  Neural Networks have practically no restriction bias, and as such have a *danger of overfitting*.

## Preference Bias

A bias in the learning algorithms selection of one representation over another, such as in decisions trees:

- preference for nodes near the top had higher information gain.
- preference of correct trees
- preference of shorter trees

In Neural Networks:

- Initial weights should be chosen to be small and random values:
	- local minima 
	- provides variability and low complexity (larger weights equate to larger complexity).

Given that we choose an algorithm with small random weights and ending on some overfitting criteria, then we can say that neural networks prefer *simpler explanations*.

## Summary

- **Perceptrons**: a linear threshold unit that can be combined together in a network to can create any Boolean function.
- **Perceptron Rule**: finite time for linearly separable input
- **General Differentiable Rule**: back propagation & gradient descent
- Preference / Restriction Bias
