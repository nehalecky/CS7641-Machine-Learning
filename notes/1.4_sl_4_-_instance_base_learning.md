# SL 4 - Instance Based Learning

## Overview

### Previously:
we used $X$ features to describe $y$, typically by searching some hypothesis space to find the best generalized function $f(x)$, ala:

$$f(x) = wx+b$$


### Now: 
within instance based learning, we create a database of all $x, y$ relationships, and upon receipt of any new value $x$, we can simply perform a lookup of $x$ to find $y$. 

$$f(x) = \textrm{lookup}(x)$$


### Advantages:

- remembers (it an exact recording of the $x$ to $y$ relationship.
- fast (does not have to perform learning)
- simple (low complexity)

### Disadvantages:

- generalization? NO
- overfitting? YES
	- believes data too much
	- lookups can result in multiple values returned
	- lookups for values that haven't been observed don't have a result.


## K Nearest Neighbors (kNN)

Look at a selection of nearest neighbors and select the majority label. 

- **Distance**: can be calculated many different ways. Depending on the problem, it needs to be adjusted to best represent the *distance* between neighboring values.

- **Similarity**: the inverse of distance.

## K-NN Algo

### Pseudocode 

#### 1. Given : 

- Training Data, $$D= \{ x_i, y_i \}$$
- Distance (Similarity) Metric, $$d(q, x)$$
- Number of Neighbors, $$k$$
- Query Point, $$q$$

#### 2. Find:

- A *set* of Nearest Neighbors, $NN$, such that,
$$NN = \{ i: d(q, x_i), k_{\textrm{smallest}} \} $$

#### 3. Return:

- **Classification**: we perform a *vote* of the $y_i \in NN$, where $y_i$ is the plurality / relative major (i.e., mathematically, the mode) of the set. 
	- Tie-breaking strategies must also be considered, as the label space is discrete (e.g., choose highest ranking mode across $D$, sort lexicographically).

- **Regression**: we compute the *mean* of the $y_i \in NN$.

### Domain Consideration

When considering all parameters previously, it's important to consider their definitions as they will have significant impact on the kNN model behavior and results. Some examples include:

- weighted vote
- weighted average (e.g., $d^{-1}$)

## QUIZ 1: Won't You Compute My Neighbors

Given $n$ sorted data points, consider learning algo and task type and give complexity for each in terms of running time (number of computations) and space (memory).

| Model | Task Type| Running Time | Space  |
| :------ | :------: | :-----: |  :-----: |
|  **1-NN** | learning   |  $1$  |   $n$ |
|  **1-NN** | query  |  $\log{n}$  | $1$ |
|  **k-NN** | learning  |  $1$  |  $n$  |
|  **k-NN** | query  |  $\log{n} + k$   |  1  |
|  **linear reg.** | learning  | $n$ |   $1$ (m, b) |
|  **linear reg.** | query  |  $1$  |  $1$  |


### Lazy vs. Eager Learners.

NN learning algos are *lazy* learning algorithms, in that they put off learning until the final moment (during the query). Conversely, linear regression is an *eager* learning algorithm, performing all learning initially when fitting $y = mx +b$.

## QUIZ 2: Domain k-NNowledge


### Consider distance metrics:

- **Manhattan**: $\ell_1$ 

$$ d = \lvert y_2 - y_1 \rvert + \lvert x_2 - x_1 \rvert  $$

- **Euclidean**: $\ell_2$ 

$$ d = \sqrt{(y_2 - y_1)^2 + (x_2 - x_1)^2} $$

**GIVEN**: 

This training dataset :

| $X$  $\in\mathbb{R}^2$	| $y$ $\in\mathbb{R}^1$ | $\ell_1$ |  $\ell_2$ | 
|:---:		| :---: | :---: | :---: | 
|	1, 6		|  	7		|   7		|	25
|	2, 4  		| 	8 		|   4		|	8
|	3, 7		| 	16 	|   6		|	 26
|	6, 8 		| 	44	|	8		|	40
|	7, 1		|	50 	|   4		|	10
|	8, 4		| 	68	|   6		|	20

and the query:

$$ q = 4, 2 $$

**COMPUTE**:

Averages, $\mu$, for the following distance type, $d_{type}$, and number nearest neighbor count, $k$.  

We find nearest (i.e., minimum $k$ values) for each distance type, and compute the average.
 

|  $d_{type}$ 	| $k$ 	| $\mu$	|
| :------ | :------: | :-----: |
| $\ell_1$   | 1 		 |	  29    |
| $\ell_1$   | 3 		 |	  35.5  |
| $\ell_2$   | 1 		 |	  8     |
| $\ell_2$   | 3		 |	  42    |

The dataset was generated using the function $y = x_1^2 + x^2$. As such, the *actual* $y$ value, given input query, $q$, is $18$.

## k-NN Bias

**Preference Bias**?

Why we prefer one hypothesis over another, defines our belief about what makes a good hypothesis.

1. **Locality**: near points are *similar* to one another
	- this aspect is embedded in the distance function selected. 
	- Some good and some bad, domain specific.
	
2. **Smoothness**: averaging

3. All features matter **equally**.

## The Curse of Dimensionality (and other Stuff)

> As the number of features (or dimensions) grows, the amount of data needed to generalize accurately grows exponentially.


## Some other stuff with KNN

- Choosing distance functions $d(x, q)$: 
- Choosing $K=n$
	- weighted averages, 
	- locally weighted regression
	- decision trees
	- neural nets

For example a *locally weighted local regression* allow for a curve fitting.

## Summary

- instance-based learning
- lazy vs. eager learning
- kNN (a lazy learning)
- nearest-neighbor: similarity (distance)
- *domain knowledge matters*!
- classification vs. regression
- averaging 
- locally weighted linear regression
- curse of dimensionality: $O(2^d)$
