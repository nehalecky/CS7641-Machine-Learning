# Supervised Learning

## Classification vs. Regression

Supervised learning allows us to build models that map some input `x` to some value. Values can be discrete, making the learning problem classification, or continuous, in which case the learning problem is regression.

### Classification

Some input `x` that maps to a label (e.g., a value belonging to a discrete set). For example `x` could be an image (e.g. photos of faces) and determining the gender of the person (e.g., `male`, `female`).


### Regression

Some input `x` could map to a real value (e.g., a value that is part of a continuous parameter) predicting the age of a person, based off of a photo of their face).

## Terminology

Multiple important terminologies exist that will be used throughout the class:

- ### Instances (Input)
Vectors of attributes that define the input space (e.g. pixel values of a picture, credit score examples containing income, event data set).

- ### Concept 
A function that maps inputs to outputs (e.g., a picture that maps to true or false). References to the more general notion of a concept by describing what something is, or is not (e.g., a set of things that are ). A mapping between objects in the world and membership in a set.

- ### Target Concept
The actual *answer* we are searching for in the space of multiple concepts, also known as the *hypothesis class*. 

- ### Hypothesis Class
The set of all concepts you're trying to entertain. Could be all possible functions, however, given finite data, an infinite space to search.

- ### Sample (Training Set)
Set of inputs and output pairs, ensuring that output is *correct* (e.g., person profile and credit worthiness).

- ### Candidate
A *concept* that is believed to be the *target concept*.

- ### Testing Set
Just like *training set*, in that it contains input-output pairs, however, represents a set that **never** overlaps with examples belonging to the *training set*.

---

# Decision Trees

## Examples

### Ex 1: Going on date with someone. 

#### Input: 
A list of restaurants with categorical labels (features):

- **Type**: Italian, French, Thai
- **Atmosphere**: Fancy, HIW, Casual
- **Busy**: Yes or No 
- **Status**: high end, average, cheap
- **Cost**: could be discrete input or an average number
- **Weather**: yes or no
- **Date Situation**: hot or not

#### Output: 
Should we stay or move on? True or False

### Ex 2: 20 questions, yes or no, 

Which allow you to guess what I'm thinking. Start with very general questions, move to more specifics as we narrow down the selection set. 

Roughly results in an algorithm that looks like:

1. Pick best attribute (where best typically is an attribute that event splits the subset of data).
2. Ask yes or no question.
3. Follow attribute path.
4. Goto 1 and repeat until you have an answer

## Presentation vs Algorithms

Decision trees are logical structures containing:

- **Nodes** represent attributes (hungry or not?)
- **Edges** represent values (yes, True or no, False)
- **Leaves** are end states, representing the output

Decision trees can be used to represent logic conditions / functions such as `OR` or `XOR`.

- **OR**, (easy, any) when mapped out in a decision tree, demonstrates *linear* complexity in number of nodes (attributes). For $n$ number of nodes, there will be $n$ operations.

- **XOR**, (hard, parity) when mapped out in a decision tree, demonstrates *exponential* complexity. For $n$ nodes, there is a bound of $2^n -1$, or $O(2^n)$.

### Parity

Even or odd parity define a boolean operation on a set of boolean data.

ODD: if number of attributes that are true is an odd number, then that the output is true, otherwise it's false.

EVEN: if number of attributes	that are true is an even number

*Note*: In general in ML, we hope to encounter problems that are more like *any* vs *parity*.

## Expressiveness of Decision Trees

Decision Trees are extremely expressive, allowing for an extremely large hypothesis space. We must have clever ways to focus search using algorithms. 

## ID3 Algorithm

A top-down decision tree learning algorithm:

### Psuedocode

Loop:

 - $A$ $\leftarrow$ best attribute
 - Assign $A$ as decision attribute for *node*
 - For each value of $A$ create a descendent of *node*.
 - Sort training examples to *leaves*.
 - IF Examples perfectly classified $\rightarrow$ **STOP**
 - ELSE Iterate over leaves.
 
### Information Gain (e.g., finding the best attribute)
 
The amount of information gained by picking a particular attribute in Decision Tree making.

Mathematically, information gain $IG$ quantifies the reduction in randomness over the labels you have with a set of data, based upon knowing the value, $v$, of a particular attribute, $A$. It is defined as:

$$\textrm{IG}(S, A) = \textrm{H}(S) - \sum_v \frac{\left|S_v\right|}{\left|S\right|} \cdot \textrm{H}(S) $$ where: 

- $v$ is an attribute value instance
- $S$ is the set of training examples and
- $A$ is an attribute.
- $H$ is the entropy, defined as

$$  H(S) = - \sum_v p(v)\log(v) $$

## ID3 Bias 

*Restriction bias* describes the hypothesis set space $H$ we will consider for the learning effort (e.g., only considering decision trees and not quadratic equations).

*Preference bias* describes the subset of hypotheses, $n$, from the hypothesis set space that we *prefer* ($n \in H$), which is really at the heart of *inductive* bias.
 
### ID3's biases:

prefers functions that:
 
1. provide good splits at the top of the tree
2. Correct over Incorrect
3. Prefers shorter trees vs. longer (due to bias #1).

## Other Considerations

### Continuous Attributes (e.g., age, weight, distance)

With continuous attributes, it can make sense to repeat asking about an attribute along a path in a decision tree. For instances, consider age as an attribute, and the desire to continually refine the range an age belongs to by asking True or False questions.

### When do we stop?

- When everything is classified correctly?
- When we've run out of attributes?
- cross validation to prevent overfitting (a big complicated tree, in the case decision trees).
- Breadth-first search

- **Pruning**, Output: Vote

### Regression

- Splitting? Variance
- Output: Average, local linear fit

 
